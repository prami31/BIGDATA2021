{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba08f89a",
   "metadata": {},
   "source": [
    "## Algoritmos en PYSPARK BOW.hs\n",
    "##### Alumna:Milagros Yarahuaman Rojas\n",
    "#### Cod:171071"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98554165",
   "metadata": {},
   "source": [
    "#### - BAGOFWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de9c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofwords(Doc):\n",
    "    \"\"\"cambia la letras a minusculas,\n",
    "    filtra letra mayores a 2,\n",
    "    quita espacios en blanco\n",
    "    Args:\n",
    "        texto (str): A string.\n",
    "    \n",
    "    Returns:\n",
    "        [[str]]: Arreglo que contiene documentos que son [str]\n",
    "    \"\"\"\n",
    "    x=Doc.strip().lower().split()\n",
    "    return list(filter(lambda x:len(x)>2,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "590a3621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['messi', 'messi', 'messi', 'ronaldo', 'ronaldo', 'balon'],\n",
       " ['messi', 'ronaldo', 'futbol', 'futbol', 'futbol']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=[\"messi messi messi ronaldo ronaldo balon\",\n",
    "          \"messi ronaldo futbol futbol futbol\"]\n",
    "Texto=sc.parallelize(A,4)\n",
    "Texto=Texto.map(bagofwords)\n",
    "Texto.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a29a1",
   "metadata": {},
   "source": [
    "#### - TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b73384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def TF(Doc):\n",
    "    \"\"\"Funcion que calcula el tf de un documento \n",
    "\n",
    "    Args:\n",
    "        Doc (str): un Str que es un documento que contiene palabras\n",
    "\n",
    "    Returns:\n",
    "        (str,double): un RDD contiene (palabra,double) tupla.\"\"\"\n",
    "    #Obtenemos el total de palabras de doc\n",
    "    total=len(Doc)\n",
    "    #Lista sin repeticiones\n",
    "    DocSR=list(set(Doc))\n",
    "    #Obtenemos el tf para cada palabra\n",
    "    mapeo=list(map(lambda x:(x,Doc.count(x)),DocSR))\n",
    "    tf=list(map(lambda x:(x[0],1+math.log10(x[1]/total)),mapeo))\n",
    "    return (tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c71082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ronaldo', 0.5228787452803375),\n",
       "  ('messi', 0.6989700043360187),\n",
       "  ('balon', 0.22184874961635637)],\n",
       " [('ronaldo', 0.30102999566398125),\n",
       "  ('futbol', 0.7781512503836436),\n",
       "  ('messi', 0.30102999566398125)]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=[\"messi messi messi ronaldo ronaldo balon\",\n",
    "          \"messi ronaldo futbol futbol futbol\"]\n",
    "Texto=sc.parallelize(A,4)\n",
    "Texto=Texto.map(bagofwords)\n",
    "TextoTF=Texto.map(TF)\n",
    "TextoTF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387377d1",
   "metadata": {},
   "source": [
    "#### - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bcb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def IDF(Doc,corpus):\n",
    "    \"\"\"Funcion que calcula el IDF de un corpus de TF \n",
    "\n",
    "    Args:\n",
    "        Doc [[(str,double)]]: texto TF\n",
    "        Doc [str]:texto corpus basico procesado con bagofwords\n",
    "\n",
    "    Returns:\n",
    "        (str,double): corpus con IDF de cada palabra sin repeticiones\"\"\"\n",
    "    DocP=Doc.flatMap(lambda x:x)\n",
    "    DocP=DocP.map(lambda x:x[0]).distinct()\n",
    "    #Total documentos en corpus\n",
    "    total=corpus.count()\n",
    "    corpus2=corpus.collect()\n",
    "    #Extraer cantida de veces de cada palabra en los documentos\n",
    "    IDF=DocP.map(lambda x:list(map(lambda y:(x,1 if x in y else 0),corpus2)))\n",
    "    IDF2=IDF.flatMap(lambda x:x)\n",
    "    IDF2=IDF2.groupByKey().map(lambda x:(x[0],sum(x[1])))\n",
    "    IDF2=IDF2.map(lambda x:(x[0],math.log10(1+total/x[1])))\n",
    "    return IDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a58b6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ronaldo', 0.3010299956639812),\n",
       " ('messi', 0.3010299956639812),\n",
       " ('balon', 0.47712125471966244),\n",
       " ('futbol', 0.47712125471966244)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=[\"messi messi messi ronaldo ronaldo balon\",\n",
    "          \"messi ronaldo futbol futbol futbol\"]\n",
    "Texto=sc.parallelize(A,4)\n",
    "Texto=Texto.map(bagofwords)\n",
    "TextoTF=Texto.map(TF)\n",
    "TextoIDF=IDF(TextoTF,Texto)\n",
    "TextoIDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af381e21",
   "metadata": {},
   "source": [
    "#### - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0391c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecIDF(word,TextoIDF):\n",
    "        \"\"\"Funcion que recuperar el valor IDF de una palabra\n",
    "        Args:\n",
    "            word str:palabra a buscar\n",
    "            Doc [(str,double)]:texto IDF\n",
    "        Returns:\n",
    "            double: IDF numerico de una palbara\"\"\"\n",
    "        L=TextoIDF.collect()\n",
    "        EL=list(filter(lambda x:x[0]==word,L))\n",
    "        return double(EL[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af710bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def TFIDF(TextoTF,TextoIDF):\n",
    "    \"\"\"Funcion que calcula el IDF para cada Doc de TF\n",
    "    Args:\n",
    "        Doc [[(str,double)]]: texto TF\n",
    "        Doc [(str,double)]:texto IDF\n",
    "\n",
    "    Returns:\n",
    "        [(str,double)]: texto con el TFIDF de cada palabra en DOC\"\"\"\n",
    "    \n",
    "    TFIDF=TextoTF.map(lambda x:list(map(lambda y:(y[0],y[1]*RecIDF(y[0],TextoIDF)),x)))\n",
    "    return  TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a32db055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.2.0-bin-hadoop3.2/python/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/home/ubuntu/spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/ubuntu/spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/ubuntu/spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\", line 262, in __getnewargs__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: RuntimeError: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# This method is called when attempting to pickle an RDD, which is always an error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;34m\"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5237/2941900877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mTextoIDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTextoTF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mTextoTFIDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTFIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTextoTF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTextoIDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mTextoTFIDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2949\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2951\u001b[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0m\u001b[1;32m   2952\u001b[0m                                       self._jrdd_deserializer, profiler)\n\u001b[1;32m   2953\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2829\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2830\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2831\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2832\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2814\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2816\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2817\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.2.0-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: RuntimeError: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "A=[\"messi messi messi ronaldo ronaldo balon\",\n",
    "          \"messi ronaldo futbol futbol futbol\"]\n",
    "Texto=sc.parallelize(A,4)\n",
    "Texto=Texto.map(bagofwords)\n",
    "TextoTF=Texto.map(TF)\n",
    "TextoIDF=IDF(TextoTF,Texto)\n",
    "TextoTFIDF=TFIDF(TextoTF,TextoIDF)\n",
    "TextoTFIDF.take(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
